xlm-roberta-large:
  model_name: "xlm-roberta-large"
  seed: 73
  train_args:
    num_epochs: 15
    train_batch_size: 32
    eval_batch_size: 32
    lr: 0.00001
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    adam_epsilon: 1.0e-8
    warmup_epoch: 1.33
    steplr_gamma: 0.5
    criterion: 'label_smoothing' # (focal, label_smoothing, f1, cross_entropy)
    scheduler_name: 'linear' # ['ReduceLROnPlateau', 'steplr', 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']
  val_args:
    use_kfold: False
    num_k: 0
    test_size: 0.1 # validation data ratio

bert-base-multilingual-cased:
  model_name: "bert-base-multilingual-cased"
  seed: 41
  train_args:
    num_epochs: 10
    train_batch_size: 16
    eval_batch_size: 16
    lr: 0.00005
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    adam_epsilon: 1.0e-8
    warmup_epoch: 3
    steplr_gamma: 0.1
    criterion: 'cross_entropy' # (focal, label_smoothing, f1, cross_entropy)
    scheduler_name: 'cosine'  
  val_args:
    use_kfold: False
    num_k: 0
    test_size: 0.1


koelectra-base-v3-discriminator:
  model_name: "monologg/koelectra-base-v3-discriminator"
  seed: 30
  train_args:
    num_epochs: 15
    train_batch_size: 16
    eval_batch_size: 16
    lr: 0.000002
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    adam_epsilon: 1.0e-8
    warmup_epoch: 3
    steplr_gamma: 0.1
    criterion: 'label_smoothing' # (focal, label_smoothing, f1, cross_entropy)
    scheduler_name: 'cosine'  
  val_args:
    use_kfold: False
    num_k: 0
    test_size: 0.1

